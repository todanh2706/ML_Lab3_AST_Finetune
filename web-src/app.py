import streamlit as st
import torch
import torch.nn as nn
import torchaudio
import soundfile as sf
import numpy as np
import io
import os
import timm
import matplotlib.pyplot as plt
import librosa.display

# --- IMPORT MODEL T·ª™ FILE LOCAL ---
from ast_models import ASTModel 

# 1. C·∫§U H√åNH & MODEL VERSIONS
st.set_page_config(page_title="UrbanSound8K Model Comparison", page_icon="üéß", layout="wide")

# ƒê·ªãnh nghƒ©a c√°c phi√™n b·∫£n Model t·∫°i ƒë√¢y
MODEL_VERSIONS = {
    "AST-P (Base384)": {
        "path": "./models/AST-P",
        "model_size": "base384", # C·∫ßn kh·ªõp v·ªõi l√∫c train (base384/base224)
        "description": "Model AST-P (Pretrained ImageNet + AudioSet)"
    },
    "AST-S (Small/Student)": {
        "path": "./models/AST-S",
        "model_size": "base384", # <--- QUAN TR·ªåNG: N·∫øu AST-S l√† ki·∫øn tr√∫c nh·ªè h∆°n, h√£y s·ª≠a th√†nh 'base224' ho·∫∑c config t∆∞∆°ng ·ª©ng
        "description": "Model AST-S (Experimental Version - Coming Soon)"
    }
}

LABELS = [
    "Air Conditioner", "Car Horn", "Children Playing", "Dog Bark",
    "Drilling", "Engine Idling", "Gun Shot", "Jackhammer",
    "Siren", "Street Music"
]

# 2. H√ÄM X·ª¨ L√ù √ÇM THANH
def make_features(waveform, sr, target_length=1024, mel_bins=128):
    target_sr = 16000
    if sr != target_sr:
        resampler = torchaudio.transforms.Resample(sr, target_sr)
        waveform = resampler(waveform)
        sr = target_sr

    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)

    num_target_samples = int(10 * sr)
    if waveform.shape[1] < num_target_samples:
        waveform = nn.functional.pad(waveform, (0, num_target_samples - waveform.shape[1]))
    else:
        waveform = waveform[:, :num_target_samples]

    fbank = torchaudio.compliance.kaldi.fbank(
        waveform, htk_compat=True, sample_frequency=sr, use_energy=False,
        window_type='hanning', num_mel_bins=mel_bins, dither=0.0, frame_shift=10
    )
    
    n_frames = fbank.shape[0]
    p = target_length - n_frames
    if p > 0:
        m = nn.ZeroPad2d((0, 0, 0, p))
        fbank = m(fbank)
    elif p < 0:
        fbank = fbank[:target_length, :]

    spectrogram_vis = fbank.t().cpu().numpy() 
    fbank = (fbank - (-4.2677393)) / (2 * 4.5689974)
    
    return fbank.unsqueeze(0), spectrogram_vis

# 3. H√ÄM LOAD MODEL ƒê·ªòNG (D·ª±a theo model_size)
# Kh√¥ng cache c·ª©ng model n·ªØa m√† cache theo tham s·ªë size ƒë·ªÉ linh ho·∫°t
@st.cache_resource
def load_architecture_skeleton(model_size='base384'):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # T·∫°o khung model d·ª±a tr√™n size ƒë∆∞·ª£c ch·ªçn
    model = ASTModel(
        label_dim=527, 
        input_tdim=1024, 
        input_fdim=128, 
        model_size=model_size, 
        audioset_pretrain=False
    )
    
    # Thay Head (Gi·ªØ nguy√™n c·∫•u tr√∫c Sequential: Norm -> Linear)
    # mlp_head l√† Sequential(LayerNorm, Linear)
    # Ch√∫ng ta ch·ªâ thay th·∫ø l·ªõp Linear cu·ªëi c√πng (index 1)
    in_features = model.mlp_head[1].in_features
    model.mlp_head[1] = nn.Linear(in_features, 10)
    
    model.to(device)
    model.eval()
    return model, device

# 4. H√ÄM PREDICT (Nh·∫≠n c·∫•u h√¨nh version)
def ensemble_predict(audio_input, version_config):
    # L·∫•y th√¥ng tin t·ª´ config
    folder_path = version_config["path"]
    model_size = version_config["model_size"]
    
    # Load ki·∫øn tr√∫c ph√π h·ª£p (Base ho·∫∑c Small)
    base_model, device = load_architecture_skeleton(model_size)
    
    # X·ª≠ l√Ω audio
    if isinstance(audio_input, bytes):
        file_obj = io.BytesIO(audio_input)
    else:
        file_obj = audio_input
        file_obj.seek(0)
        
    try:
        # S·ª≠ d·ª•ng soundfile ƒë·ªÉ ƒë·ªçc file thay v√¨ torchaudio.load
        audio_data, sr = sf.read(file_obj)
        
        # Chuy·ªÉn ƒë·ªïi sang Tensor c·ªßa PyTorch
        # soundfile tr·∫£ v·ªÅ (samples, channels) ho·∫∑c (samples,)
        # torchaudio mong ƒë·ª£i (channels, samples)
        waveform = torch.tensor(audio_data, dtype=torch.float32)
        
        if waveform.ndim == 1:
            waveform = waveform.unsqueeze(0) # (1, samples)
        else:
            waveform = waveform.t() # (channels, samples)
            
    except Exception as e:
        st.error(f"L·ªói ƒë·ªçc file √¢m thanh: {str(e)}")
        return None, 0, np.zeros(10), None
    input_tensor, spec_vis = make_features(waveform, sr)
    input_tensor = input_tensor.to(device)
    
    sum_probs = torch.zeros(1, 10).to(device)
    
    # T·∫°o progress bar ri√™ng cho t·ª´ng l·∫ßn ch·∫°y
    progress_text = st.empty()
    bar = st.progress(0)
    
    valid_models_count = 0
    
    # Duy·ªát qua 10 fold trong folder t∆∞∆°ng ·ª©ng
    for i in range(1, 11):
        # C·∫≠p nh·∫≠t t√™n file theo format m·ªõi c·ªßa b·∫°n: ast_us8k_foldX.pth
        pth_path = os.path.join(folder_path, f"ast_us8k_fold{i}.pth")
        
        if not os.path.exists(pth_path): 
            continue
            
        valid_models_count += 1
        progress_text.text(f"ƒêang ch·∫°y Fold {i} t·ª´ {folder_path}...")
        bar.progress(i * 10)
        
        try:
            state_dict = torch.load(pth_path, map_location=device)
            new_state_dict = {}
            for k, v in state_dict.items():
                name = k.replace("module.", "") 
                new_state_dict[name] = v
                
            base_model.load_state_dict(new_state_dict, strict=True)
            
            with torch.no_grad():
                output = base_model(input_tensor)
                probs = torch.softmax(output, dim=-1)
                sum_probs += probs
                
        except Exception as e:
            st.warning(f"L·ªói t·∫£i {pth_path}: {e}")

    bar.empty()
    progress_text.empty()
    
    if valid_models_count == 0:
        st.error(f"Kh√¥ng t√¨m th·∫•y file model n√†o trong th∆∞ m·ª•c: {folder_path}")
        return None, 0, np.zeros(10), spec_vis

    avg_probs = sum_probs / valid_models_count # Chia cho s·ªë model th·ª±c t·∫ø t√¨m th·∫•y
    final_idx = avg_probs.argmax(-1).item()
    
    return LABELS[final_idx], avg_probs[0][final_idx].item(), avg_probs[0].cpu().numpy(), spec_vis

# --- GIAO DI·ªÜN CH√çNH ---
st.title("üéß UrbanSound8K Analysis System")
st.markdown("---")

# 1. Sidebar ch·ªçn Model
st.sidebar.header("‚öôÔ∏è C·∫•u h√¨nh Model")
selected_version_name = st.sidebar.selectbox(
    "Ch·ªçn phi√™n b·∫£n Model:",
    list(MODEL_VERSIONS.keys())
)
current_config = MODEL_VERSIONS[selected_version_name]

st.sidebar.info(f"**M√¥ t·∫£:** {current_config['description']}")
st.sidebar.warning(f"**ƒê∆∞·ªùng d·∫´n:** `{current_config['path']}`")

# 2. Input Audio
col1, col2 = st.columns([1, 2])

with col1:
    st.subheader("1. Nh·∫≠p li·ªáu")
    tab_up, tab_mic = st.tabs(["üìÇ Upload File", "üéôÔ∏è Ghi √¢m"])
    input_data = None

    with tab_up:
        up_file = st.file_uploader("Ch·ªçn file .wav", type=["wav"])
        if up_file: input_data = up_file.read()

    with tab_mic:
        mic_file = st.audio_input("B·∫Øt ƒë·∫ßu ghi √¢m")
        if mic_file: input_data = mic_file

with col2:
    st.subheader("2. K·∫øt qu·∫£ ph√¢n t√≠ch")
    if input_data:
        # N√∫t b·∫•m ph√¢n t√≠ch
        if st.button("üöÄ Ch·∫°y m√¥ h√¨nh " + selected_version_name, type="primary"):
            with st.spinner(f"ƒêang x·ª≠ l√Ω v·ªõi {selected_version_name}..."):
                lbl, conf, probs, spec_img = ensemble_predict(input_data, current_config)
                
                if lbl:
                    # H√†ng 1: K·∫øt qu·∫£ text + Audio player
                    r1_c1, r1_c2 = st.columns(2)
                    with r1_c1:
                        st.success(f"D·ª± ƒëo√°n: **{lbl}**")
                        st.metric("ƒê·ªô tin c·∫≠y", f"{conf:.1%}")
                    with r1_c2:
                        st.audio(input_data, format='audio/wav')

                    # H√†ng 2: Spectrogram + Bi·ªÉu ƒë·ªì c·ªôt
                    st.write("---")
                    sub_c1, sub_c2 = st.columns([3, 2])