\pagebreak
\section{Thử nghiệm của nhóm}

\subsection{Môi trường thực nghiệm}

Các thí nghiệm được triển khai trong môi trường notebook trực tuyến có hỗ trợ GPU với nền tảng CUDA
(Kaggle Notebook). Mô hình được hiện thực bằng thư viện PyTorch (phiên bản 2.x),
kết hợp cùng các thư viện torchaudio, timm, librosa và scikit-learn để xử lý tín hiệu
và xây dựng pipeline huấn luyện. Các phép tính trên GPU được tăng tốc bằng cơ chế
mixed precision training thông qua mô-đun torch.amp.autocast và GradScaler, giúp
giảm dung lượng bộ nhớ và rút ngắn thời gian huấn luyện.

Dữ liệu UrbanSound8K được mount sẵn tại thư mục /kaggle/input và toàn bộ
mã nguồn, checkpoint của mô hình được lưu tại /kaggle/working.

\subsection{Tiền xử lý tín hiệu âm thanh}

Mọi đoạn âm thanh trong UrbanSound8K đều được chuyển về dạng log Mel filterbank
(fbank) trước khi đưa vào mô hình Audio Spectrogram Transformer (AST). Quy trình
tiền xử lý gồm các bước:

\begin{itemize}
    \item Tải tín hiệu âm thanh bằng torchaudio với dạng tensor (channels, time).
    \item Nếu tín hiệu có nhiều kênh (stereo), chuyển về mono bằng cách lấy trung bình
    trên trục kênh.
    \item Chuẩn hóa tần số lấy mẫu về 16 kHz bằng phép nội suy lại (resampling).
    \item Cắt hoặc pad tín hiệu về đúng 10 giây, tương ứng với một số mẫu cố định.
    \item Trích xuất đặc trưng log Mel filterbank với 128 băng tần Mel, window 25 ms,
    bước trượt 10 ms, thu được khoảng 1024 khung thời gian.
    \item Nếu số khung thời gian nhỏ hơn 1024 thì pad thêm khung 0; nếu lớn hơn thì
    cắt bớt cho vừa 1024 khung.
    \item Chuẩn hóa đặc trưng theo công thức
    \((x - \mu) / (2 \sigma)\) với \(\mu = -4.2677\) và \(\sigma = 4.5690\),
    nhằm đưa về miền giá trị phù hợp với mô hình AST gốc.
\end{itemize}

Kết quả mỗi đoạn âm thanh là một tensor kích thước (T, F) = (1024, 128), sau đó
được đưa vào mô hình dưới dạng batch.

\subsection{Cấu hình mô hình Audio Spectrogram Transformer}

Trong tất cả các thí nghiệm, nhóm sử dụng cùng một kiến trúc Audio Spectrogram
Transformer (AST) với cấu hình base384, stride thời gian và tần số đều bằng 10.
Với đầu vào 128 băng tần Mel và 1024 khung thời gian, mô hình tạo ra 1212 patch
spectrogram. Phần đầu ra của backbone là một vector đặc trưng có chiều 768, sau đó
được đưa qua một lớp phân loại tuyến tính (mlp head) ánh xạ về 10 lớp
tương ứng với 10 loại âm thanh của UrbanSound8K.

Trên cơ sở kiến trúc chung này, nhóm xây dựng hai cấu hình tiền huấn luyện khác nhau:

\subsubsection{Cấu hình AST-S (AST-ImageNet)}

Trong kịch bản thứ nhất, mô hình AST được khởi tạo từ các trọng số đã được huấn luyện
trước trên ImageNet thông qua tham số imagenet\_pretrain = True và audioset\_pretrain = False trong hàm khởi tạo ASTModel. Toàn bộ backbone vì vậy kế thừa khả năng trích xuất đặc trưng từ ảnh (được ánh xạ sang miền spectrogram), nhưng không sử dụng thêm bất kì checkpoint nào từ AudioSet.\\
Lớp phân loại gốc của AST (với 527 nút đầu ra) được thay thế bằng một lớp tuyến tính mới có 10 nút, khởi tạo ngẫu nhiên, để phù hợp với bài toán phân loại 10 lớp trên UrbanSound8K. Cấu hình này được ký hiệu là AST-S.

\subsubsection{Cấu hình AST-P (AST-ImageNet, AudioSet)}

Trong kịch bản thứ hai, mô hình AST được khởi tạo sao cho tương thích với checkpoint đã fine-tune trên AudioSet (audioset\_0.4593.pth). Cụ thể, mô hình được tạo với label\_dim = 527, input\_tdim = 1024, input\_fdim = 128, và cả hai tham số imagenet\_pretrain, audioset\_pretrain đều đặt False để nhóm chủ động nạp trọng số từ checkpoint AudioSet bên ngoài.

Sau khi tải file trọng số audioset\_0.4593.pth, toàn bộ state\_dict được nạp vào mô hình, giúp backbone kế thừa trực tiếp khả năng phân biệt âm thanh từ tập AudioSet. Tương tự như cấu hình trước, lớp phân loại 527 chiều được thay bằng một lớp tuyến tính 10 chiều, khởi tạo ngẫu nhiên cho UrbanSound8K. Cấu hình này được ký hiệu là AST-P.\\
Hai cấu hình trên sử dụng chung toàn bộ pipeline tiền xử lý, kiến trúc backbone, hàm mất mát, optimizer và chiến lược huấn luyện; điểm khác biệt duy nhất nằm ở nguồn trọng số tiền huấn luyện được sử dụng cho backbone.

\subsection{Chiến lược huấn luyện và đánh giá}

Để đánh giá mô hình một cách công bằng, nhóm áp dụng chiến lược 10-fold cross-validation theo đúng cách chia fold có sẵn trong UrbanSound8K:

\begin{itemize}
    \item Ở mỗi vòng lặp, chọn một fold (từ 1 đến 10) làm tập kiểm thử.
    \item Chín fold còn lại được gộp lại tạo thành tập huấn luyện.
    \item Tuyệt đối không trộn mẫu giữa các fold, đảm bảo không xảy ra rò rỉ dữ liệu.
\end{itemize}

Với mỗi cấu hình mô hình (AST-S và AST-P), nhóm sử dụng cùng một bộ
siêu tham số:

\begin{itemize}
    \item Hàm mất mát: Cross-entropy cho bài toán phân loại đơn nhãn.
    \item Optimizer: AdamW với learning rate cố định là \(1 \times 10^{-5}\).
    \item Kích thước batch: 8 (được lựa chọn để phù hợp với dung lượng bộ nhớ GPU).
    \item Số epoch: từ 3 đến 5 epoch cho mỗi fold, tùy theo giới hạn tính toán; trong cả hai
    cấu hình, mô hình hội tụ khá nhanh nhờ sử dụng trọng số tiền huấn luyện.
    \item Huấn luyện sử dụng mixed precision (autocast và GradScaler) để giảm chi phí tính toán.
\end{itemize}

Sau mỗi epoch, mô hình được đánh giá trên tập kiểm thử của fold tương ứng. Nhóm
lưu lại checkpoint có độ chính xác cao nhất trên tập kiểm thử (best test accuracy) cho mỗi fold. Các checkpoint này được ghi vào thư mục ast\_us8k\_checkpoints.\\
Cuối cùng, để tổng hợp kết quả, nhóm tính toán độ chính xác trung bình và độ lệch chuẩn trên toàn bộ 10 fold, theo công thức:

\[
\text{Mean Accuracy} = \frac{1}{10} \sum_{k=1}^{10} \text{Acc}_k,
\quad
\text{Std} = \sqrt{ \frac{1}{10} \sum_{k=1}^{10} (\text{Acc}_k - \text{Mean Accuracy})^2 }.
\]

Việc sử dụng cùng một thiết lập huấn luyện và cùng chiến lược cross-validation cho cả AST-S và AST-P giúp việc so sánh trực tiếp tác động của tiền huấn luyện
trên AudioSet đối với hiệu năng phân loại âm thanh môi trường trên UrbanSound8K.

\subsection{Lí do chọn cấu hình}
\subsubsection{Phân tích EDA dữ liệu}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{img/1_statistic_distribution.png}
    \caption{Thống kê phân phối mẫu của các lớp}
    \label{label:stats_distribution}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{img/2_waveform_spectrogram.png}
    \caption{Waveform và spectrogram của các lớp}
    \label{label:wavef_spectrogram}
\end{figure}
Kết quả phân tích dữ liệu khám phá (EDA) và rà soát kịch bản huấn luyện (training script) hiện tại cho thấy cấu hình tham số (hyperparameters) đang được thiết lập chưa tương thích tốt với đặc điểm phân bố của dữ liệu. Điều này dẫn đến sự lãng phí tài nguyên tính toán và nguy cơ mô hình không đạt được điểm hội tụ tối ưu (suboptimal convergence).\\
Từ các biểu đồ thống kê và trực quan hóa tín hiệu trên, hai đặc điểm cốt lõi của tập dữ liệu UrbanSound8K đã được xác định:
\begin{itemize}
    \item \textbf{Phân bố thời gian (Temporal Distribution):} Biểu đồ histogram cho thấy mật độ tập trung cao của các mẫu dữ liệu ở độ dài $4.0$ giây. Rất ít mẫu vượt quá ngưỡng này.
    \item \textbf{Sự mất cân bằng dữ liệu (Class Imbalance):} Tồn tại sự chênh lệch đáng kể về số lượng mẫu giữa các lớp. Cụ thể, các lớp như \texttt{car\_horn} và \texttt{gun\_shot} có số lượng mẫu thấp hơn khoảng $60\%$ so với các lớp đa số (như \texttt{engine\_idling}, \texttt{jackhammer}).
\end{itemize}
Khác với các tập dữ liệu hình ảnh thông thường (nơi các ảnh thường độc lập với nhau), UrbanSound8K có đặc thù về nguồn gốc dữ liệu. Cụ thể, dữ liệu là các file âm thanh ngắn (slices) được cắt ra từ các bản thu âm dài gốc. Nếu chia tập Train/Test một cách ngẫu nhiên (random split), rất có thể các đoạn cắt (slices) từ cùng một bản thu âm gốc sẽ nằm rải rác ở cả tập Train và tập Test, từ đó mô hình sẽ học thuộc lòng đặc trưng nền (background noise) của bản thu gốc thay vì học đặc trưng của loại âm thanh đó. Điều này dẫn đến kết quả kiểm thử cao giả tạo nhưng mô hình thực tế lại hoạt động kém. Việc sử dụng cột fold có sẵn trong file metadata (được thiết kế sao cho các đoạn cắt từ cùng một nguồn luôn nằm chung một fold) là biện pháp kỹ thuật bắt buộc để đảm bảo tính độc lập dữ liệu (Data Independence).\\
Tuy nhiên, cấu hình huấn luyện hiện tại được đánh giá là chưa tối ưu dựa trên các luận điểm khoa học sau:
\paragraph{Lãng phí tài nguyên tính toán do padding quá nhiều} Hiện tại, mô hình đang thiết lập độ dài đầu vào là 1024 frames (tương đương khoảng 10 giây). Tuy nhiên, thực tế dữ liệu chỉ dài trung bình 4 giây, điều này buộc hệ thống phải thực hiện kỹ thuật Zero-Padding (chèn các giá trị 0) vào khoảng $60\%$ kích thước của tensor đầu vào. Từ đó, cơ chế Self-Attention của Transformer phải thực hiện tính toán trên một vùng không gian thưa (sparse) chứa ít thông tin hữu ích. Điều này không chỉ gây lãng phí bộ nhớ GPU và thời gian huấn luyện mà còn có thể gây nhiễu cho quá trình trích xuất đặc trưng.
\paragraph{Sai lệch mô hình do mất cân bằng dữ liệu} Việc không áp dụng các kỹ thuật cân bằng dữ liệu (Data Balancing) trong bối cảnh tập dữ liệu bị lệch (skewed dataset) khiến hàm mất mát (Loss function) sẽ bị chi phối bởi các lớp đa số. Từ đó, mô hình có xu hướng tối ưu hóa độ chính xác toàn cục bằng cách hy sinh khả năng nhận diện các lớp thiểu số (\texttt{gun\_shot}, \texttt{car\_horn}), dẫn đến chỉ số F1-Score thấp mặc dù Accuracy có thể cao.
