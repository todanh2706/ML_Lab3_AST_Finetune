\pagebreak
\section{Các công trình liên quan}
Lịch sử phát triển của bài toán phân loại âm thanh và nhận dạng sự kiện âm thanh (Audio Event Detection - AED) là một quá trình tiến hóa liên tục nhằm tìm kiếm các biểu diễn đặc trưng (feature representations) hiệu quả hơn. Quá trình này có thể được chia thành ba giai đoạn chính: từ các mô hình thống kê dựa trên đặc trưng thủ công, đến các mạng nơ-ron tích chập (CNN) và các biến thể lai ghép, và gần đây nhất là sự trỗi dậy của kiến trúc Transformer thuần túy.
\subsection{Phương pháp thống kê và mô hình GMM-HMM}
Trong giai đoạn đầu, các hệ thống nhận dạng âm thanh chủ yếu dựa vào các đặc trưng được thiết kế thủ công (hand-crafted features) như Mel-Frequency Cepstral Coefficients (MFCCs) hay Filterbanks. Mô hình tiêu biểu nhất trong giai đoạn này là sự kết hợp giữa \textbf{Mô hình Hỗn hợp Gaussian (Gaussian Mixture Models - GMM)} và \textbf{Mô hình Markov Ẩn (Hidden Markov Models - HMM)} \cite{80902}.\\
Về mặt lý thuyết, GMM được sử dụng để mô hình hóa phân phối xác suất của các đặc trưng quang phổ tại mỗi khung thời gian (acoustic modeling), trong khi HMM chịu trách nhiệm mô hình hóa sự phụ thuộc và chuyển đổi trạng thái theo trình tự thời gian (temporal modeling). Mặc dù GMM-HMM đã đặt nền móng vững chắc cho ngành xử lý tiếng nói, chúng tồn tại những hạn chế cố hữu:
\begin{itemize}
    \item \textbf{Phụ thuộc vào trích xuất đặc trưng:} Hiệu suất mô hình bị giới hạn bởi chất lượng của các đặc trưng thủ công, vốn không thể nắm bắt hết sự phức tạp của tín hiệu âm thanh thực tế.
    \item \textbf{Giả định độc lập:} HMM dựa trên giả định Markov, cho rằng trạng thái hiện tại chỉ phụ thuộc vào trạng thái liền trước, do đó gặp khó khăn trong việc nắm bắt các phụ thuộc dài hạn (long-term dependencies) trong chuỗi tín hiệu.
\end{itemize}
\subsection{Mạng Nơ-ron Tích chập (CNN) và Kiến trúc Lai ghép (Hybrid Models)}
Sự ra đời của AlexNet \cite{krizhevsky2012imagenet} vào năm 2012 đã đánh dấu sự chuyển dịch sang kỷ nguyên học sâu (Deep Learning). Các nhà nghiên cứu bắt đầu tiếp cận âm thanh dưới dạng hình ảnh thông qua biểu đồ phổ (Spectrogram), cho phép áp dụng các kiến trúc Mạng nơ-ron tích chập (CNN) mạnh mẽ như VGG hay ResNet để tự động học các đặc trưng từ dữ liệu thay vì thiết kế thủ công.\\
CNN sở hữu thiên kiến quy nạp (inductive bias) mạnh mẽ, bao gồm tính cục bộ (locality) và tính bất biến tịnh tiến (translation equivariance), giúp chúng rất hiệu quả trong việc phát hiện các mẫu hình tần số cục bộ. Tuy nhiên, CNN thuần túy gặp hạn chế trong việc mô hình hóa chuỗi thời gian toàn cục do giới hạn của vùng tiếp nhận (receptive field). Để khắc phục, các kiến trúc Lai ghép (Hybrid Models) đã được đề xuất:
\begin{itemize}
    \item \textbf{CRNN (Convolutional Recurrent Neural Networks):} Kết hợp CNN (để trích xuất đặc trưng không gian) với RNN hoặc LSTM (để mô hình hóa chuỗi thời gian).
    \item \textbf{CNN-Attention:} Thay thế hoặc bổ sung cơ chế RNN bằng cơ chế Chú ý (Attention Mechanism) để cho phép mô hình tập trung vào các đoạn âm thanh quan trọng và tổng hợp thông tin tốt hơn.
\end{itemize}
Mặc dù các mô hình lai ghép này đã đạt được những kết quả vượt trội, chúng vẫn chịu ràng buộc bởi cấu trúc tích chập: khả năng nắm bắt thông tin toàn cục chỉ đạt được ở các lớp rất sâu hoặc thông qua các phép toán gộp (pooling) làm mất mát thông tin chi tiết.
\subsection{Kiến trúc Transformer và Vision Transformer (ViT)}
Kiến trúc Transformer, ban đầu được giới thiệu cho các tác vụ xử lý ngôn ngữ tự nhiên, dựa hoàn toàn vào cơ chế Tự chú ý (Self-Attention) để mô hình hóa các mối quan hệ toàn cục trong chuỗi dữ liệu. Dosovitskiy et al. (2020) đã đề xuất Vision Transformer (ViT) \cite{dosovitskiy2020vit}, minh chứng rằng một kiến trúc Transformer thuần túy có thể đạt hiệu suất vượt trội trong thị giác máy tính bằng cách chia hình ảnh thành chuỗi các mảnh ghép (patches) cố định (ví dụ: $16 \times 16$) và xử lý chúng tương tự như các từ trong câu.\\
Khác biệt cốt lõi của ViT so với CNN nằm ở Vùng tiếp nhận toàn cục (Global Receptive Field). Ngay từ lớp đầu tiên, cơ chế Self-Attention cho phép mỗi mảnh ghép tham chiếu thông tin từ tất cả các mảnh ghép khác, giúp mô hình nắm bắt cấu trúc tổng thể của dữ liệu ngay lập tức mà không cần trải qua nhiều lớp tích chập phân cấp. Hơn nữa, ViT có thiên kiến quy nạp yếu hơn, cho phép mô hình linh hoạt hơn trong việc học các mối quan hệ phức tạp từ các tập dữ liệu quy mô lớn.
\subsection{Audio Spectrogram Transformer (AST)}
Kế thừa thành tựu của ViT, Gong et al. (2021) đã đề xuất mô hình Audio Spectrogram Transformer (AST) \cite{gong2021ast}. Đây là kiến trúc đầu tiên áp dụng cơ chế Transformer không tích chập (convolution-free) cho bài toán phân loại âm thanh.\\
AST xử lý biểu đồ phổ âm thanh đầu vào (kích thước $128 \times 100t$) bằng cách chia nhỏ thành chuỗi các mảnh $16 \times 16$ có sự chồng lấn (overlap). Các mảnh này được chiếu tuyến tính thành các vector nhúng (embeddings), cộng với thông tin vị trí (positional embedding) và đưa qua bộ mã hóa Transformer chuẩn. Điểm đột phá của AST nằm ở khả năng tận dụng tri thức chuyển giao (Transfer Learning) từ miền hình ảnh (ImageNet) sang miền âm thanh. Các tác giả đã đề xuất các kỹ thuật thích ứng hiệu quả như trung bình hóa trọng số kênh và nội suy vị trí để giải quyết sự khác biệt về chiều dữ liệu giữa hai miền.\\
Các thực nghiệm trên AudioSet và ESC-50 cho thấy AST không chỉ vượt qua các mô hình CNN-Attention lai ghép tốt nhất (SOTA) mà còn chứng minh khả năng hội tụ nhanh và hiệu quả dữ liệu vượt trội nhờ vào cơ chế chú ý toàn cục và chiến lược khởi tạo trọng số thông minh.